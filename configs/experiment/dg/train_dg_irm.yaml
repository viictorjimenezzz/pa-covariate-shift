# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data/dg@data: wilds_multienv.yaml
  - override /model/dg@model: irm.yaml
  - override /trainer: ddp.yaml


# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

name: ???
tags: ["wilds", "irm", "${name}"]

seed: 123

trainer:
  min_epochs: 100
  max_epochs: 100
  gradient_clip_val: 0.0

model:
  optimizer:
    _target_: torch.optim.SGD
    lr: 0.0001
  net:
    pretrained: false
    net: resnet18
    n_classes: ${data.dg.n_classes}

data:
  # datasets_dir: ${paths.data_dir}/dg/dg_datasets/rebuttal/
  dataset_dir: ${paths.data_dir}/dg/dg_datasets/wilds/
  # collate_fn:
  #   _target_: hydra.utils.get_method # to avoid instantiation of a callable
  #   path: src.data.components.collate_functions.MultiEnv_collate_fn
  batch_size: 64
  num_workers: 2
  pin_memory: true
  # root_dir: '/cluster/project/jbuhmann/posterior_agreement/adv_pa/data/diagvib/domain_shift' # data_dir is specified in config.yaml


logger:
  wandb:
    tags: ${tags}
    entity: entity-name
    project: project-name
    group: group-name
    save_dir: ${paths.output_dir}/dg
    name: ${name}