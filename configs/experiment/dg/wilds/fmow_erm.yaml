# @package _global_

# Source: WILDS paper & code
# https://arxiv.org/pdf/2012.07421.pdf
# https://github.com/p-lambda/wilds/blob/472677590de351857197a9bf24958838c39c272b/examples/configs/datasets.py

defaults:
  - /experiment/dg/wilds/wilds.yaml
  - /model/dg@model: erm.yaml

# The original batch size is 32, we will use 4 with 8 gradient accumulation steps
data:
  batch_size: 2
trainer:
  max_epochs: 60
  accumulate_grad_batches: 16 # 8

model:
  optimizer:
    _target_: torch.optim.Adam
    lr: 0.0001
    weight_decay: 0.0
  net:
    pretrained: true
    net: densenet121
    n_classes: ${data.n_classes}

  scheduler:
    interval: epoch
    frequency: 1

    scheduler:
      _target_: torch.optim.lr_scheduler.StepLR
      _partial_: true
      gamma: 0.96
      step_size: 1

tags: 
  - "dg"
  - "wilds"
  - "${data.dataset_name}"
  - "erm"
  - "${model.net.net}"
  - "${name_logger}"
  - "${classname: ${model.optimizer._target_}}"