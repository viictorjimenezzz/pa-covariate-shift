# @package _global_

defaults:
  - override /data/dg: diagvib_two.yaml
  - override /model/dg: optimize.yaml
  - override /callbacks:
    - model_summary.yaml
    - rich_progress_bar.yaml
  - override /trainer: ddp.yaml

model:
  dg:
    optimizer:
      _target_: torch.optim.Adam
    classifier:
      exp_name: ???
      net:
        net: resnet18
        pretrained: false

tags: ["diagvib", "adam", "500_epochs", "${model.dg.classifier.exp_name}"]

seed: 12345

trainer:
  min_epochs: 500
  max_epochs: 500
  log_every_n_steps: 10
  devices: 1

data:
  dg:
    datasets_dir: ${paths.data_dir}/dg/dg_datasets/paper_replication/
    collate_fn:
      _target_: hydra.utils.get_method
      path: src.data.components.collate_functions.IRM_collate_fn
    shift_ratio: ???
    batch_size: 100
    num_workers: 40
    pin_memory: True

logger:
  wandb:
    tags: ${tags}
    entity: entity-name
    project: project-name
    group: group-name
    save_dir: ${paths.output_dir}/dg
    name: model=${model.dg.classifier.exp_name}_${data.dg.env1_name}_vs_${data.dg.env2_name}_sr=${data.dg.shift_ratio}