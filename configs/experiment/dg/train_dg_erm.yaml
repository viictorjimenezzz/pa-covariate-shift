# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data/dg@data: wilds_multienv.yaml
  - override /model/dg@model: erm.yaml
  - override /trainer: ddp.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

name: ???
tags: ["wilds", "erm", "${name}"]

seed: 123

trainer:
  min_epochs: 100
  max_epochs: 100
  gradient_clip_val: 0.0

model:
  optimizer:
    _target_: torch.optim.SGD
    lr: 0.0001
  net:
    pretrained: false
    net: resnet18
    n_classes: ${data.dg.n_classes}

  scheduler: null

data:
  dataset_dir: ${paths.data_dir}/dg/dg_datasets/wilds/
  batch_size: 64
  num_workers: 2
  pin_memory: true


logger:
  wandb:
    tags: ${tags}
    entity: entity-name
    project: project-name
    group: group-name
    save_dir: ${paths.output_dir}/dg
    name: ${name}


