# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data/adv: cifar10.yaml
  - override /model/adv: optimize.yaml
  - override /callbacks:
    - model_summary.yaml
    - rich_progress_bar.yaml
  - override /trainer: ddp.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

model:
  adv:
    classifier: ${data.adv.classifier}
    optimizer:
      _target_: torch.optim.Adam

tags: ["order_by_attack", "adam", "500_epochs", "1000_steps", "cifar10", "${data.adv.classifier.model_name}", "${data.adv.attack.attack_name}"]

seed: 12345

trainer:
  min_epochs: 500
  max_epochs: 500
  log_every_n_steps: 3
  devices: 1

data:
  num_workers: 40
  pin_memory: True

logger:
  wandb:
    tags: ${tags}
    entity: entity-name
    project: project-name
    group: group-name
    save_dir: ${paths.output_dir}/adv
    name: att=${data.adv.attack.attack_name}_mod=${data.adv.classifier.model_name}_ar=${data.adv.adversarial_ratio}
